# ğŸ—ï¸ Architecture Documentation

## System Overview

Smart RAG Chatbot is designed with a modular, production-ready architecture that separates concerns and enables easy maintenance and extension.

## Core Components

### 1. RAG Engine (`main.py`)

The heart of the system, implementing intelligent routing and conversation management.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         User Question                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Question Classifier (LLM)            â”‚
â”‚    - Casual vs Knowledge                 â”‚
â”‚    - Context-aware classification        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚
       â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Casual Mode  â”‚  â”‚ Knowledge    â”‚
â”‚ (Direct LLM) â”‚  â”‚ Mode (RAG)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Retriever   â”‚
                  â”‚  (VectorDB)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ LLM Generate â”‚
                  â”‚ with Context â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. LLM Provider Abstraction (`llm_provider.py`)

Enables seamless switching between Ollama (local) and OpenAI (cloud).

```python
get_llm() â†’ Returns appropriate LLM instance
get_embeddings() â†’ Returns appropriate embeddings instance
```

**Benefits:**
- Easy provider switching
- Consistent interface
- Environment-based configuration

### 3. Document Processing Pipeline (`ingestion.py`)

Handles document loading, chunking, and vectorization.

```
Document â†’ Load â†’ Chunk â†’ Embed â†’ Store â†’ Retrieve
```

**Supported Formats:**
- PDF
- TXT
- DOCX
- Web URLs

### 4. Utilities Layer

#### a. Caching (`utils/cache.py`)
- Thread-safe in-memory cache
- TTL support
- Decorator-based usage

#### b. Logging (`utils/logger.py`)
- Structured logging (JSON/Console)
- Context enrichment
- Multiple handlers

#### c. Metrics (`utils/metrics.py`)
- Request tracking
- Latency measurement
- Error counting
- Cache hit rates

#### d. Retry & Circuit Breaker (`utils/retry.py`)
- Exponential backoff
- Configurable retry logic
- Circuit breaker pattern

#### e. Validation (`utils/validation.py`)
- Input sanitization
- Security checks
- Type validation

## Data Flow

### Conversation with Memory

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Conversation Memory              â”‚
â”‚  [user: "What is X?",                   â”‚
â”‚   assistant: "X is...",                 â”‚
â”‚   user: "How does it work?"]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    New Question + Context                â”‚
â”‚    "How does it work?" + memory         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Context-Aware Processing              â”‚
â”‚    (Knows "it" refers to X)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Question   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embed Question  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Similarity       â”‚
â”‚  Search (top-k docs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rank & Filter           â”‚
â”‚  (relevance check)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Augment Prompt          â”‚
â”‚  (question + context)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Generate            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Post-process            â”‚
â”‚  (add sources, etc.)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration System

Environment-based configuration with validation:

```python
Config (Pydantic BaseModel)
â”œâ”€â”€ LLM Provider Settings
â”œâ”€â”€ Embedding Settings
â”œâ”€â”€ Vector Store Settings
â”œâ”€â”€ System Settings
â””â”€â”€ Logging Settings
```

**Features:**
- Type checking
- Value validation
- Default values
- Environment variable loading

## Error Handling Strategy

```
Exception Hierarchy:
AdvancedRAGException (base)
â”œâ”€â”€ ConfigurationError
â”œâ”€â”€ RetrievalError
â”œâ”€â”€ VectorStoreError
â”œâ”€â”€ GenerationError
â”œâ”€â”€ WebSearchError
â”œâ”€â”€ GradingError
â”œâ”€â”€ ValidationError
â”œâ”€â”€ CacheError
â””â”€â”€ RetryExhaustedError
```

Each exception includes:
- Detailed message
- Context dictionary
- Logging integration

## Performance Optimizations

### 1. Caching
- Function-level caching with decorators
- LRU eviction policy
- TTL support

### 2. Batch Processing
- Document loading in batches
- Parallel embedding (when supported)

### 3. Lazy Loading
- LLM models loaded on first use
- Vector store initialized on demand

### 4. Memory Management
- Conversation memory limited to last N messages
- Automatic cleanup of expired cache entries

## Security Considerations

### 1. Input Validation
- XSS prevention
- SQL injection prevention
- Path traversal prevention

### 2. API Key Management
- Environment variables only
- Never logged or exposed

### 3. Rate Limiting
- Circuit breaker for external APIs
- Configurable retry limits

## Extensibility Points

### Adding New LLM Providers

```python
# In llm_provider.py
if config.llm_provider == "new_provider":
    return NewProviderLLM(...)
```

### Adding New Document Loaders

```python
# In ingestion.py or load_custom_docs.py
if file_ext == ".new_format":
    loader = NewFormatLoader(file_path)
```

### Adding New Features

The modular design makes it easy to add:
- New conversation modes
- Additional grading steps
- Custom retrieval strategies
- Alternative vector stores

## Testing Strategy

### Unit Tests
- Individual component testing
- Mocked dependencies
- Edge case coverage

### Integration Tests
- End-to-end flows
- Real LLM interactions (optional)
- Vector store operations

## Deployment Considerations

### Development
```bash
LOG_LEVEL=DEBUG
LOG_FORMAT=console
LLM_PROVIDER=ollama
```

### Production
```bash
LOG_LEVEL=INFO
LOG_FORMAT=json
LLM_PROVIDER=openai  # or optimized Ollama setup
ENABLE_CACHE=true
```

## Monitoring & Observability

### Metrics Collected
- Request count (total/success/failure)
- Latency (avg/min/max per operation)
- Cache hit rate
- Error counts by type
- LLM token usage (if tracked)

### Logging Levels
- DEBUG: Detailed execution flow
- INFO: Key operations and results
- WARNING: Degraded performance or recoverable errors
- ERROR: Operation failures
- CRITICAL: System-level failures

## Future Architecture Enhancements

### Planned
1. **Async/Await Support**
   - Non-blocking I/O
   - Concurrent request handling

2. **Distributed Caching**
   - Redis integration
   - Shared cache across instances

3. **Database Integration**
   - Persistent conversation history
   - User management

4. **API Layer**
   - FastAPI REST endpoints
   - WebSocket support for streaming

5. **Multi-tenancy**
   - Isolated vector stores per user
   - Resource quotas

---

*This architecture is designed to be simple enough for understanding but robust enough for production use.*

