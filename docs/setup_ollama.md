# ğŸš€ Ollama Kurulum ve Ã‡alÄ±ÅŸtÄ±rma Rehberi

## AdÄ±m 1: Ollama Kurulumu

### Windows
1. **Ä°ndir**: https://ollama.com/download/windows
2. **Kur**: `OllamaSetup.exe` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
3. **Kontrol**: Terminal'de ÅŸunu Ã§alÄ±ÅŸtÄ±r:
   ```bash
   ollama --version
   ```

### Kurulum BaÅŸarÄ±lÄ± mÄ±?
EÄŸer `ollama version x.x.x` gibi bir Ã§Ä±ktÄ± gÃ¶rÃ¼yorsanÄ±z, baÅŸarÄ±lÄ±! âœ…

---

## AdÄ±m 2: Model Ä°ndirme

### Ã–nerilen Model: Llama 3.1 8B (TÃ¼rkÃ§e DesteÄŸi Harika!)

```bash
# Model'i indir (yaklaÅŸÄ±k 4.7 GB)
ollama pull llama3.1:8b

# Embedding model'i indir (Ã¶nemli!)
ollama pull nomic-embed-text
```

### Alternatif Modeller

**Daha KÃ¼Ã§Ã¼k (Daha HÄ±zlÄ±)**:
```bash
ollama pull llama3.2:3b  # 2GB - Hafif sistemler iÃ§in
```

**Daha Ä°yi Kalite (Daha YavaÅŸ)**:
```bash
ollama pull llama3.1:70b  # 40GB - GÃ¼Ã§lÃ¼ sistemler iÃ§in
```

---

## AdÄ±m 3: Model Test Et

```bash
# Model'i Ã§alÄ±ÅŸtÄ±r
ollama run llama3.1:8b

# Åunu yaz:
>>> Merhaba, nasÄ±lsÄ±n?

# TÃ¼rkÃ§e cevap gelirse baÅŸarÄ±lÄ±! âœ…
# Ã‡Ä±kmak iÃ§in: /bye
```

---

## AdÄ±m 4: Server BaÅŸlat

Ollama otomatik olarak arka planda Ã§alÄ±ÅŸÄ±r, ama manuel baÅŸlatmak isterseniz:

```bash
ollama serve
```

**Port**: http://localhost:11434

---

## AdÄ±m 5: Python KurulumlarÄ±

```bash
# Gerekli paketleri yÃ¼kle
pip install langchain-community

# Test et
python llm_provider.py
```

---

## ğŸ› Sorun Giderme

### "Ollama is not running"
```bash
# Windows'ta Ollama'yÄ± baÅŸlat
# GÃ¶rev Ã‡ubuÄŸunda Ollama ikonunu arayÄ±n
# Veya terminalde:
ollama serve
```

### Model indirilemedi
```bash
# Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin
# Tekrar deneyin:
ollama pull llama3.1:8b --insecure
```

### Port 11434 kullanÄ±mda
```bash
# BaÅŸka bir port kullan
ollama serve --port 11435

# .env dosyasÄ±nÄ± gÃ¼ncelle:
OLLAMA_BASE_URL=http://localhost:11435
```

---

## âœ… Kurulum KontrolÃ¼

Her ÅŸey tamam mÄ± kontrol et:

```bash
# 1. Ollama Ã§alÄ±ÅŸÄ±yor mu?
ollama list

# 2. Modeller inmiÅŸ mi?
# Åunu gÃ¶rmelisin:
# llama3.1:8b
# nomic-embed-text

# 3. Python test
python -c "from langchain_community.llms import Ollama; print('OK')"
```

Hepsi âœ… ise, hazÄ±rsÄ±n! ğŸ‰

---

## ğŸ“Š Sistem Gereksinimleri

| Model | RAM | Disk | GPU |
|-------|-----|------|-----|
| llama3.1:8b | 8GB | 5GB | Ä°steÄŸe baÄŸlÄ± |
| llama3.2:3b | 4GB | 2GB | Ä°steÄŸe baÄŸlÄ± |
| llama3.1:70b | 64GB | 40GB | Ã–nerilir |

---

## ğŸš€ Sonraki AdÄ±mlar

Kurulum tamamsa:

```bash
# .env dosyasÄ±nÄ± kopyala
copy .env.local .env

# Test et
python llm_provider.py

# RAG sistemini Ã§alÄ±ÅŸtÄ±r
python main.py
```


